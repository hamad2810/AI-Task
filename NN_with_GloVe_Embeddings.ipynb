{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN with GloVe Embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVWDmKSuHab_"
      },
      "source": [
        "## Neural Network with LSTMs for Text Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDGPNDLe4LHX"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SNTdpK91h_C",
        "outputId": "333c1895-f482-4a46-9de8-5a5c4e151c1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df = pd.read_excel('SORs.xlsx')\n",
        "df.head()\n",
        "dataList = df['Observation Details'].to_list()\n",
        "print(len(dataList))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m52n5IqTcUmH"
      },
      "source": [
        "vocab_size = 10000\n",
        "embedding_dim = 100\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type = \"post\"\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qAHpPUQG7_i"
      },
      "source": [
        "# Pre-Processing \n",
        "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(dataList)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(dataList)\n",
        "padded = pad_sequences(sequences,padding=padding_type, maxlen=max_length, truncating=trunc_type)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJvwsNNnG-CP"
      },
      "source": [
        "print(len(word_index))\n",
        "print(word_index['and'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn8xYt7kHAZy"
      },
      "source": [
        "# Note this is the 100 dimension version of GloVe from Stanford\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n",
        "    -O /tmp/glove.6B.100d.txt\n",
        "embeddings_index = {};\n",
        "with open('/tmp/glove.6B.100d.txt') as f:\n",
        "    for line in f:\n",
        "        values = line.split();\n",
        "        word = values[0];\n",
        "        coefs = np.asarray(values[1:], dtype='float32');\n",
        "        embeddings_index[word] = coefs;\n",
        "\n",
        "embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word);\n",
        "    if embedding_vector is not None:\n",
        "        embeddings_matrix[i] = embedding_vector;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hbAa1WEHHEh"
      },
      "source": [
        "print(len(embeddings_matrix))\n",
        "print(len(embeddings_index['and'])) # 100 dimensional Embedding vector "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihfrrrW-1ga0"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n",
        "    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n",
        "    tf.keras.layers.MaxPooling1D(pool_size=4),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(units=120, activation='relu')\n",
        "])\n",
        "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "num_epochs = 100\n",
        "history = model.fit(padded, padded, epochs=num_epochs,validation_data=(padded, padded), verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvGtbqcdJXEW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Use matplotlib in notebook output\n",
        "%matplotlib inline\n",
        "loss=history.history['loss']\n",
        "epochs=range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'r')\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}